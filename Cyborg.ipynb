{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPgS9hPBxDtaxC8DGKSnOah",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/belal-bh/cyborg/blob/main/Cyborg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CyborgGPT"
      ],
      "metadata": {
        "id": "2nFeRFK9SCc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cyborg** is a LLM Bot `CyborgGPT` that can be used as a `secure` asistance in your meetings/recorded conversations along with relevant documents, giving you ability to chat on the context of Knowledge Bage (**KB**) created from your data (`meetings/recorded conversations along with relevant documents`). It has no ability to send your data over internet. That's why it is `secure/private/local`\n",
        "\n",
        "**NOTES:**\n",
        "- This colab notebook currently `only` work for `GPU Runtime`. It'will be updated later for `CPU Rintime` too.\n",
        "- It is prototype and will be improved day by day.\n",
        "- Please follow the instructions [Cyborg Prototype Video Link](https://drive.google.com/drive/folders/1K6VYiQBlaDWizJCNxNHfVh3rxm5MC_AD?usp=sharing) to run without error OR as expected.\n"
      ],
      "metadata": {
        "id": "MSj-tpULMChr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare"
      ],
      "metadata": {
        "id": "iI6TrXyaUL0k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgZrqdO_V3d1",
        "outputId": "a0e94a24-8498-41b5-fd98-a8ca8b11c098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cyborg'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 56 (delta 20), reused 47 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (56/56), 12.71 KiB | 765.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# clone the original repogitory\n",
        "!git clone https://github.com/belal-bh/cyborg.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see the list of files in current directory\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9Qi4NAGWYN9",
        "outputId": "8026fd04-ea94-4073-af4f-41e65316b1d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cyborg\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move cloned repository to the current working directory\n",
        "# (Note: hidden files would not be moved)\n",
        "!mv -f cyborg/* .\n",
        "!ls"
      ],
      "metadata": {
        "id": "7ZxYJaD-Wh4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c2b002-7366-49a2-9fd1-5ff73e1a0c48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACKNOWLEDGEMENT.md  cyborgGPT.py       get_retrieval_qa.py  sample_data\n",
            "audios\t\t    default_config.py  load_model.py\t    transcriptions\n",
            "config.py\t    documents\t       README.md\t    utils.py\n",
            "cyborg\t\t    generateKB.py      requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required python libraries\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmy6XZsHXHZw",
        "outputId": "7d2a4d2d-4785-4fe2-9629-080ebe6ca6ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai-whisper (from -r requirements.txt (line 2))\n",
            "  Downloading openai-whisper-20230314.tar.gz (792 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.9/792.9 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langchain (from -r requirements.txt (line 3))\n",
            "  Downloading langchain-0.0.202-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb (from -r requirements.txt (line 4))\n",
            "  Downloading chromadb-0.3.26-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting InstructorEmbedding (from -r requirements.txt (line 5))\n",
            "  Downloading InstructorEmbedding-1.0.1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting sentence-transformers (from -r requirements.txt (line 6))\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-cpu (from -r requirements.txt (line 9))\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface_hub (from -r requirements.txt (line 10))\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers (from -r requirements.txt (line 11))\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (3.20.3)\n",
            "Collecting auto-gptq (from -r requirements.txt (line 13))\n",
            "  Downloading auto_gptq-0.2.2.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting llama-cpp-python (from -r requirements.txt (line 15))\n",
            "  Downloading llama_cpp_python-0.1.63.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfminer.six (from -r requirements.txt (line 16))\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m131.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (3.0.10)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 20)) (1.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (1.26.15)\n",
            "Collecting accelerate (from -r requirements.txt (line 22))\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes (from -r requirements.txt (line 23))\n",
            "  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (8.1.3)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (0.56.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (4.65.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper->-r requirements.txt (line 2)) (9.1.0)\n",
            "Collecting tiktoken==0.3.1 (from openai-whisper->-r requirements.txt (line 2))\n",
            "  Downloading tiktoken-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0 (from openai-whisper->-r requirements.txt (line 2))\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python==0.2.0->openai-whisper->-r requirements.txt (line 2)) (0.18.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.1->openai-whisper->-r requirements.txt (line 2)) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.1->openai-whisper->-r requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper->-r requirements.txt (line 2)) (3.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper->-r requirements.txt (line 2)) (3.12.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper->-r requirements.txt (line 2)) (16.0.5)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (2.0.10)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain->-r requirements.txt (line 3))\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain->-r requirements.txt (line 3))\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain->-r requirements.txt (line 3))\n",
            "  Downloading dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.9 (from langchain->-r requirements.txt (line 3))\n",
            "  Downloading langchainplus_sdk-0.0.10-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (2.8.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain->-r requirements.txt (line 3))\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (1.10.7)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 3)) (8.2.2)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 4)) (1.5.3)\n",
            "Collecting requests>=2.26.0 (from tiktoken==0.3.1->openai-whisper->-r requirements.txt (line 2))\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hnswlib>=0.7 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading clickhouse_connect-0.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.1/965.1 kB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 4)) (0.7.1)\n",
            "Collecting fastapi>=0.85.1 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading fastapi-0.97.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->-r requirements.txt (line 4)) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading pulsar_client-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading onnxruntime-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting overrides>=7.3.1 (from chromadb->-r requirements.txt (line 4))\n",
            "  Downloading overrides-7.3.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 6)) (0.15.2+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 6)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 6)) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 6)) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers->-r requirements.txt (line 6))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r requirements.txt (line 10)) (2023.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r requirements.txt (line 10)) (23.1)\n",
            "Collecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 11))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from auto-gptq->-r requirements.txt (line 13))\n",
            "  Downloading datasets-2.13.0-py3-none-any.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge (from auto-gptq->-r requirements.txt (line 13))\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python->-r requirements.txt (line 15))\n",
            "  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->-r requirements.txt (line 16)) (2.0.12)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->-r requirements.txt (line 16)) (40.0.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->-r requirements.txt (line 17)) (1.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 22)) (5.9.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3))\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3))\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3))\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb->-r requirements.txt (line 4)) (2022.12.7)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb->-r requirements.txt (line 4)) (2022.7.1)\n",
            "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect>=0.5.7->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->-r requirements.txt (line 16)) (1.15.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain->-r requirements.txt (line 3))\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain->-r requirements.txt (line 3))\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain->-r requirements.txt (line 3))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.85.1->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (23.3.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (1.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 4)) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->-r requirements.txt (line 2)) (3.1.2)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading httptools-0.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (414 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.1/414.1 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq->-r requirements.txt (line 13)) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets->auto-gptq->-r requirements.txt (line 13))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets->auto-gptq->-r requirements.txt (line 13))\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->auto-gptq->-r requirements.txt (line 13))\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper->-r requirements.txt (line 2)) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper->-r requirements.txt (line 2)) (67.7.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers->-r requirements.txt (line 6)) (8.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->-r requirements.txt (line 16)) (2.21)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb->-r requirements.txt (line 4)) (3.6.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->-r requirements.txt (line 3))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper->-r requirements.txt (line 2)) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb->-r requirements.txt (line 4)) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper, sentence-transformers, auto-gptq, llama-cpp-python, hnswlib\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230314-py3-none-any.whl size=796910 sha256=d334bb80842cc3f52bf623238ef1b3f27a91ac5c389bb8e08ac3e5a337573e91\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/13/5f/fe8245f6dc59df505879da4b2129932e342f02a80e6b87f27d\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=f34caa4224766341ac42884f56f63cc5a4f33e554a45a92d93e129a183416d38\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for auto-gptq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for auto-gptq: filename=auto_gptq-0.2.2-cp310-cp310-linux_x86_64.whl size=3651744 sha256=e6bbd7f20d63f2f2f4b78949d1eda4eb780bf7ff46c7b42dbfb1b5b1ac04191f\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/f6/50/bb6ab784e7824cbf190a1a8205d91e6543287718fb21d5b033\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.63-cp310-cp310-linux_x86_64.whl size=259134 sha256=50316f120fcc01eb536b85640bcd95bcdef78eb4b742c590e345fb7e13ff3a08\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/c9/cb/ebc9d1bf7e7651737b43dcf027d408f21465c5fea38aa306ed\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2119879 sha256=d0edb7bc1a2e785a9d23c3268810be3e806eb8bb1fd3872920ea34ee4562ae1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "Successfully built openai-whisper sentence-transformers auto-gptq llama-cpp-python hnswlib\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, monotonic, InstructorEmbedding, faiss-cpu, bitsandbytes, zstandard, xxhash, websockets, uvloop, rouge, requests, python-dotenv, pulsar-client, overrides, mypy-extensions, multidict, marshmallow, lz4, humanfriendly, httptools, hnswlib, h11, frozenlist, ffmpeg-python, diskcache, dill, backoff, async-timeout, yarl, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, openapi-schema-pydantic, multiprocess, marshmallow-enum, llama-cpp-python, langchainplus-sdk, huggingface_hub, coloredlogs, clickhouse-connect, aiosignal, transformers, pdfminer.six, onnxruntime, fastapi, dataclasses-json, aiohttp, langchain, chromadb, datasets, accelerate, sentence-transformers, openai-whisper, auto-gptq\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed InstructorEmbedding-1.0.1 accelerate-0.20.3 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 auto-gptq-0.2.2 backoff-2.2.1 bitsandbytes-0.39.0 chromadb-0.3.26 clickhouse-connect-0.6.3 coloredlogs-15.0.1 dataclasses-json-0.5.8 datasets-2.13.0 dill-0.3.6 diskcache-5.6.1 faiss-cpu-1.7.4 fastapi-0.97.0 ffmpeg-python-0.2.0 frozenlist-1.3.3 h11-0.14.0 hnswlib-0.7.0 httptools-0.5.0 huggingface_hub-0.15.1 humanfriendly-10.0 langchain-0.0.202 langchainplus-sdk-0.0.10 llama-cpp-python-0.1.63 lz4-4.3.2 marshmallow-3.19.0 marshmallow-enum-1.5.1 monotonic-1.6 multidict-6.0.4 multiprocess-0.70.14 mypy-extensions-1.0.0 onnxruntime-1.15.0 openai-whisper-20230314 openapi-schema-pydantic-1.2.4 overrides-7.3.1 pdfminer.six-20221105 posthog-3.0.1 pulsar-client-3.2.0 python-dotenv-1.0.0 requests-2.31.0 rouge-1.0.1 safetensors-0.3.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 starlette-0.27.0 tiktoken-0.3.1 tokenizers-0.13.3 transformers-4.30.2 typing-inspect-0.9.0 uvicorn-0.22.0 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3 xxhash-3.2.0 yarl-1.9.2 zstandard-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Audios"
      ],
      "metadata": {
        "id": "Ro6CDR3TR7-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload your `meetings/conversation aodios or record` in the `audios` directory."
      ],
      "metadata": {
        "id": "WshOJQjsQPM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload aditional documents if need"
      ],
      "metadata": {
        "id": "jA3ONYPeSNiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relavent documents can be uploaded in the `documents` directory PDF, TXT, CSV files are supported."
      ],
      "metadata": {
        "id": "SLpY24C2SVOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Knowledge Base (KB)"
      ],
      "metadata": {
        "id": "4dXBAaepSzKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It will create a `KB` form your uploaded data. First time it will take time because of some module will be downloaded if required."
      ],
      "metadata": {
        "id": "bl37vmRYS5qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python generateKB.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Jb00Gbvxt6r",
        "outputId": "0d167248-a4d1-4fcb-b374-8e7e26c8c816"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:01<00:00, 93.6MiB/s]\n",
            "audio_files [PosixPath('/content/audios/aboutwhisper.m4a'), PosixPath('/content/audios/english.wav')]\n",
            "Detected language: en\n",
            "The Whisper models are trained for specific admission, transmission, capital of transcribed beings as a video into a text into language of Swarakan. As well as translated into English.\n",
            "Detected language: en\n",
            "Hello, can you hear me? And testing with Spurs CLI? Okay, first start.\n",
            "Loading documents from [PosixPath('/content/transcriptions'), PosixPath('/content/documents')] folders\n",
            "source_dirs [PosixPath('/content/transcriptions'), PosixPath('/content/documents')]\n",
            "Loading document batch\n",
            "Loading document batch\n",
            "Loaded 3 documents from  [PosixPath('/content/transcriptions'), PosixPath('/content/documents')] folders\n",
            "Split into 74 chunks of text\n",
            "Downloading (…)c7233/.gitattributes: 100% 1.48k/1.48k [00:00<00:00, 6.80MB/s]\n",
            "Downloading (…)_Pooling/config.json: 100% 270/270 [00:00<00:00, 1.16MB/s]\n",
            "Downloading (…)/2_Dense/config.json: 100% 116/116 [00:00<00:00, 538kB/s]\n",
            "Downloading pytorch_model.bin: 100% 3.15M/3.15M [00:00<00:00, 98.8MB/s]\n",
            "Downloading (…)9fb15c7233/README.md: 100% 66.3k/66.3k [00:00<00:00, 39.5MB/s]\n",
            "Downloading (…)b15c7233/config.json: 100% 1.53k/1.53k [00:00<00:00, 10.0MB/s]\n",
            "Downloading (…)ce_transformers.json: 100% 122/122 [00:00<00:00, 580kB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.34G/1.34G [00:04<00:00, 286MB/s]\n",
            "Downloading (…)nce_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 242kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 10.7MB/s]\n",
            "Downloading spiece.model: 100% 792k/792k [00:00<00:00, 423MB/s]\n",
            "Downloading (…)c7233/tokenizer.json: 100% 2.42M/2.42M [00:00<00:00, 2.88MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 2.41k/2.41k [00:00<00:00, 13.8MB/s]\n",
            "Downloading (…)15c7233/modules.json: 100% 461/461 [00:00<00:00, 2.56MB/s]\n",
            "load INSTRUCTOR_Transformer\n",
            "2023-06-16 17:57:26.890590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "max_seq_length  512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat with CyborgGPT"
      ],
      "metadata": {
        "id": "C5V-f3POTQuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now `KB` is ready and you are able to ask relavant questions with `CyborgGPT`"
      ],
      "metadata": {
        "id": "AqIkpRqATXcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python cyborgGPT.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nuq84yuVAm4L",
        "outputId": "d4cdb73d-1359-414e-f128-093fd8ddf8ec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-16 17:58:18.803592: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on: cuda\n",
            "Display Source Documents set to: False\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "Loading Model: TheBloke/WizardLM-7B-uncensored-GPTQ, on: cuda\n",
            "This action can take a few minutes!\n",
            "Using AutoGPTQForCausalLM for quantized models\n",
            "Downloading (…)okenizer_config.json: 100% 727/727 [00:00<00:00, 3.60MB/s]\n",
            "Downloading tokenizer.model: 100% 500k/500k [00:00<00:00, 36.6MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 2.92MB/s]\n",
            "Downloading (…)in/added_tokens.json: 100% 21.0/21.0 [00:00<00:00, 130kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 96.0/96.0 [00:00<00:00, 591kB/s]\n",
            "Tokenizer loaded\n",
            "Downloading (…)lve/main/config.json: 100% 552/552 [00:00<00:00, 3.38MB/s]\n",
            "Downloading (…)quantize_config.json: 100% 57.0/57.0 [00:00<00:00, 288kB/s]\n",
            "Downloading (…)ct-order.safetensors: 100% 3.89G/3.89G [00:18<00:00, 205MB/s]\n",
            "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
            "The safetensors archive passed at /root/.cache/huggingface/hub/models--TheBloke--WizardLM-7B-uncensored-GPTQ/snapshots/dcb3400039f15cff76b43a4921c59d47c5fc2252/WizardLM-7B-uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
            "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n",
            "Downloading (…)neration_config.json: 100% 137/137 [00:00<00:00, 634kB/s]\n",
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n",
            "The model 'LlamaGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
            "Local LLM Loaded\n",
            "----------------------------------Cyborg---------------------------\n",
            "Starting QA Session with Cyborg...(Type 'exit' to end session)...\n",
            "...................................................................\n",
            "\n",
            "Enter a query: what is the constitution about?\n",
            "\n",
            "\n",
            "> Question:\n",
            "what is the constitution about?\n",
            "\n",
            "> Answer:\n",
            " The Constitution of the United States is a governing document that outlines the rules and principles for the operation of the federal government. It includes provisions for the creation and maintenance of a democratic system, the separation of powers, and the protection of individual liberties.\n",
            "\n",
            "Enter a query: exit\n",
            "...................................................................\n",
            "Cyborg: Ending Session, Good Luck!...\n",
            "----------------------------------Cyborg---------------------------\n"
          ]
        }
      ]
    }
  ]
}